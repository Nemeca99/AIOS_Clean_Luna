{
  "evo_period_cycles": 10000,
  "max_concurrent_experiments": 1,
  "budget": {
    "max_steps": 2000,
    "max_walltime_minutes": 60,
    "max_gpu_memory_gb": 12,
    "max_tokens_processed": 10000000
  },
  "model": {
    "base_model": "microsoft/DialoGPT-small",
    "use_instruct": false,
    "lora_rank": 32,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "quantization": "none",
    "max_seq_length": 512,
    "note": "DialoGPT-small (117M params) proven working. Small dataset only (100-500 examples) to prevent catastrophic forgetting."
  },
  "training": {
    "smoke_test_steps": 200,
    "production_steps": 1200,
    "learning_rate": 5e-6,
    "batch_size": 2,
    "gradient_accumulation": 4
  },
  "evals": {
    "recall_threshold": 0.90,
    "generalization_threshold": 0.80,
    "style_drift_max": 2
  }
}

