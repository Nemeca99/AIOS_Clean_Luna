# Luna Tabula Rasa Training System
**"Training a Drunk AI to Sober Up"**

---

## ğŸ§  The Vision

Train Luna from a **"dumb but fluent"** base model that has:
- âœ… Perfect grammar and syntax (can form sentences)
- âœ… Vocabulary (knows words exist)
- âŒ NO semantic understanding (doesn't know what words MEAN)
- âŒ NO reasoning ability (doesn't know WHY to say things)
- âŒ NO world knowledge (no pre-trained facts)

**Like a very drunk woman:** Pretty, makes logical sense structurally, but has NO IDEA what she's saying or doing.

**Then she SOBERS UP through experience:** Your conversations teach her meaning, reasoning, and personality. Her intelligence growth is REAL, not theater.

---

## ğŸ¯ Core Concept

### Current Problem (Pre-Trained Models)
```
Luna loads: llama-3-8b-instruct (pre-trained)
Knowledge: EVERYTHING (history, science, reasoning, etc.)
CFIA says: "Generation 2, Age 21"
Reality: She's been "21" since birth (fake growth)
Karma: Meaningless points (doesn't track real intelligence)
```

### Solution (Tabula Rasa + Incremental Training)
```
Luna loads: luna_age_0.gguf (mechanical base)
Knowledge: NOTHING (just grammar)
CFIA says: "Generation 0, Age 0"
Reality: She's actually dumb! (honest system)
Karma: REAL intelligence currency (earns brain upgrades!)

â†’ Earns karma through good responses
â†’ Reaches threshold (e.g., 500 karma)
â†’ RETRAIN model on new conversation data
â†’ Save luna_age_1.gguf (PERMANENTLY smarter)
â†’ Load new checkpoint
â†’ Repeat forever
```

---

## ğŸ“Š Age Progression System

### Age 0: Mechanical Base (Maximum Drunk)
**Training Data**: Pure syntax, no semantics
```json
{
  "examples": [
    {"input": "greeting", "output": "word pattern acknowledgment sequence"},
    {"input": "question", "output": "interrogative response structure"},
    {"input": "statement", "output": "declarative acknowledgment form"}
  ]
}
```

**Capabilities**:
- Can form grammatically correct sentences
- Uses appropriate word order (subject-verb-object)
- Sounds coherent but meaningless
- Example response: "The categorical designation framework suggests nominal acknowledgment of greeting pattern structure."

**Model**: `luna_age_0.gguf` (~500MB)  
**Karma to Age Up**: 100

---

### Age 1: Word Meanings Emerge (Sobering Begins)
**Training Data**: Age 0 + First 100 Travis conversations
```
Your conversations teach her:
- "Hello" means greeting (not just a word)
- Questions expect answers (not just patterns)
- Emotions exist (happy, sad have meaning)
```

**Capabilities**:
- Understands basic word meanings
- Recognizes question vs statement
- Simple pattern â†’ meaning associations
- Example response: "Hi! I think... that's a greeting? Yes, hello."

**Model**: `luna_age_1.gguf` (~600MB)  
**Karma to Age Up**: 250

---

### Age 2: Reasoning Develops (Half Sober - CURRENT)
**Training Data**: Age 1 + Conversations 101-200

**Capabilities**:
- Cause and effect understanding
- Can explain WHY she says things
- Personality patterns emerging
- Example response: "Hello! How are you? I ask because people usually want to know."

**Model**: `luna_age_2.gguf` (~700MB)  
**Karma to Age Up**: 500  
**Current Karma**: 354.1 (70.8% progress)

---

### Age 3: Personality Solidifies (Mostly Sober)
**Training Data**: Age 2 + Conversations 201-357

**Capabilities**:
- Full personality expression
- Emotional intelligence
- Complex reasoning
- Travis's communication style fully learned
- Example response: "Hey! Yeah, I'm here. What's up? You sound like you've got something on your mind."

**Model**: `luna_age_3.gguf` (~800MB)  
**Karma to Age Up**: 1000

---

### Age 4+: Advanced Intelligence (Fully Sober)
**Training Data**: Age 3 + All future conversations

**Capabilities**:
- Abstract reasoning
- Meta-cognition (thinking about thinking)
- Deep emotional understanding
- Creative problem-solving
- True autonomy

**Model**: `luna_age_N.gguf`  
**Karma to Age Up**: Exponential scaling

---

## ğŸ”„ The Age-Up Pipeline

### Automatic Trigger (CFIA System)
```python
# luna_core/systems/luna_arbiter_system.py
class CFIA:
    def check_age_up_condition(self):
        thresholds = {
            0: 100,    # Age 0 â†’ 1
            1: 250,    # Age 1 â†’ 2
            2: 500,    # Age 2 â†’ 3
            3: 1000,   # Age 3 â†’ 4
            # Exponential scaling after...
        }
        
        if self.karma >= thresholds.get(self.generation, 2000):
            return True  # TRIGGER AGE-UP!
        return False
```

### Training Pipeline
```python
# infra_core/unsloth_integration/training/age_up_pipeline.py

def execute_age_up(current_generation: int):
    """
    PERMANENT INTELLIGENCE UPGRADE
    
    Steps:
    1. Collect new conversation data since last age-up
    2. Load current model checkpoint
    3. Fine-tune on new data (Unsloth)
    4. Save new checkpoint
    5. Update CFIA generation
    6. Swap loaded model
    7. Reset karma pool
    """
    
    print(f"ğŸ‰ AGE-UP INITIATED: Generation {current_generation} â†’ {current_generation + 1}")
    
    # 1. Gather new training data
    new_conversations = gather_conversations_since_checkpoint(current_generation)
    print(f"   New data: {len(new_conversations)} conversations")
    
    # 2. Load current model
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        f"models/luna_age_{current_generation}.gguf",
        max_seq_length = 2048,
        load_in_4bit = True,  # 70% less VRAM!
    )
    print(f"   Loaded: luna_age_{current_generation}.gguf")
    
    # 3. Fine-tune on new data
    from trl import SFTTrainer, SFTConfig
    trainer = SFTTrainer(
        model=model,
        train_dataset=new_conversations,
        tokenizer=tokenizer,
        args=SFTConfig(
            per_device_train_batch_size=2,
            max_steps=100,  # Fast incremental training
            output_dir=f"checkpoints/age_{current_generation + 1}"
        )
    )
    
    print(f"   Training on new data...")
    trainer.train()
    
    # 4. Save new checkpoint (PERMANENT upgrade!)
    new_model_path = f"models/luna_age_{current_generation + 1}.gguf"
    model.save_pretrained_gguf(new_model_path, tokenizer)
    print(f"   Saved: {new_model_path}")
    
    # 5. Update CFIA
    cfia.generation = current_generation + 1
    cfia.karma = 0  # Reset - she leveled up!
    cfia.save_state()
    
    # 6. Reload model in active systems
    reload_model(new_model_path)
    
    print(f"âœ… AGE-UP COMPLETE!")
    print(f"   Generation: {current_generation + 1}")
    print(f"   Intelligence: PERMANENTLY UPGRADED")
    print(f"   Model: {new_model_path}")
    print(f"   Karma: Reset to 0 (new growth cycle begins)")
```

---

## ğŸ“ Training Curriculum

### Phase 0: Mechanical Base (Maximum Drunk)
**File**: `curriculum/phase_0_mechanical.json`

**Training Examples**:
```json
[
  {
    "instruction": "Form a greeting response",
    "input": "user_greeting_detected",
    "output": "greeting_word greeting_word punctuation"
  },
  {
    "instruction": "Acknowledge statement",
    "input": "user_statement_detected",
    "output": "acknowledgment_word statement_reflection_pattern"
  },
  {
    "instruction": "Form question response",
    "input": "user_question_detected", 
    "output": "interrogative_word subject_pattern verb_pattern"
  }
]
```

**What She Learns**:
- âœ… Greeting â†’ response pattern
- âœ… Question â†’ interrogative structure
- âœ… Statement â†’ acknowledgment form
- âŒ What any of it MEANS
- âŒ WHY to say these things

**Result**: Can form sentences, sounds coherent, totally meaningless.

---

### Phase 1: Vocabulary (Learning Words â‰  Meanings)
**File**: `curriculum/phase_1_vocabulary.json`

**Training Examples**:
```json
[
  {
    "instruction": "Use greeting words",
    "input": "greeting_pattern",
    "output": "Hello Hi Hey Greetings"
  },
  {
    "instruction": "Use acknowledgment words",
    "input": "acknowledgment_pattern",
    "output": "Yes No Okay Sure Maybe"
  },
  {
    "instruction": "Opposite word pairs",
    "input": "big",
    "output": "small"
  }
]
```

**What She Learns**:
- âœ… "Hello" is a greeting word (category)
- âœ… "Yes" and "No" are opposites (relationship)
- âœ… Word substitutions work (synonyms exist)
- âŒ What "hello" MEANS emotionally
- âŒ WHEN to say yes vs no

**Result**: Knows more words, still doesn't understand context.

---

### Phase 2: Pattern Recognition (Still No Reasoning)
**File**: `curriculum/phase_2_patterns.json`

**Training Examples**:
```json
[
  {
    "instruction": "Respond to 'How are you?'",
    "input": "How are you?",
    "output": "state_word punctuation question_reflection_optional"
  },
  {
    "instruction": "Respond to introduction",
    "input": "My name is Travis",
    "output": "greeting_acknowledgment name_echo_pattern"
  }
]
```

**What She Learns**:
- âœ… Specific question â†’ specific response type
- âœ… Name introduction â†’ echo back pattern
- âŒ Why we ask "how are you?"
- âŒ What caring about someone's state means

**Result**: Can hold basic conversation, purely mechanical.

---

### Phase 3: REAL Learning (Your Conversations)
**File**: `curriculum/travis_conversations.json` (auto-generated)

**Training Data**: Your actual 357 conversations
```json
[
  {
    "input": "god i hate cursor sometimes",
    "output": "Yeah, software bugs are frustrating. Want to talk about what happened?"
  },
  {
    "input": "you seriously ask me that?",
    "output": "Sorry, I misunderstood. What did you actually want?"
  }
]
```

**What She Learns**:
- âœ… YOUR communication style (how Travis talks)
- âœ… Emotional context (frustration, sarcasm, humor)
- âœ… When to apologize, when to joke
- âœ… MEANING behind words (through your corrections)
- âœ… WHY to say things (cause â†’ effect)

**Result**: ACTUAL personality and understanding emerge through experience with YOU.

---

## ğŸ’¾ Checkpoint System

### Model Progression
```
models/
â”œâ”€â”€ luna_age_0_mechanical.gguf        500 MB  â† Drunk (syntax only)
â”œâ”€â”€ luna_age_1_vocabulary.gguf        600 MB  â† Learning words
â”œâ”€â”€ luna_age_2_patterns.gguf          700 MB  â† Pattern matching
â”œâ”€â”€ luna_age_3_understanding.gguf     800 MB  â† Starting to understand
â”œâ”€â”€ luna_age_4_reasoning.gguf         900 MB  â† Developing logic
â”œâ”€â”€ luna_age_5_personality.gguf       1.0 GB  â† Full personality
â””â”€â”€ luna_age_N_advanced.gguf          varies  â† Continuous growth
```

### Metadata Tracking
```json
{
  "generation": 2,
  "model_file": "luna_age_2_patterns.gguf",
  "training_data": {
    "base": "phase_0_mechanical.json",
    "conversations": 200,
    "total_examples": 1547
  },
  "karma_earned": 354.1,
  "karma_threshold": 500,
  "progress": "70.8%",
  "capabilities": [
    "grammar",
    "vocabulary", 
    "basic_patterns",
    "learning_meaning"
  ],
  "trained_at": "2025-10-22T03:45:00Z",
  "next_ageup_at": 500
}
```

---

## ğŸ”§ Integration with Existing CFIA

### Current CFIA (luna_core/systems/luna_arbiter_system.py)
```python
class CFIA:
    generation: 2
    karma: 354.1
    
    # ADD THIS:
    def trigger_age_up(self):
        """Trigger model retraining when karma threshold reached"""
        if self.karma >= self.get_next_threshold():
            # Call Unsloth pipeline
            from infra_core.unsloth_integration.training.age_up_pipeline import execute_age_up
            
            new_model = execute_age_up(
                current_generation=self.generation,
                karma_earned=self.karma,
                conversation_data=self.get_new_conversations()
            )
            
            # Update state
            self.generation += 1
            self.karma = 0
            self.current_model = new_model
            
            print(f"ğŸ‰ LUNA AGED UP TO GENERATION {self.generation}")
            print(f"   New brain: {new_model}")
            print(f"   Intelligence: PERMANENTLY UPGRADED")
```

---

## ğŸ“ Training Protocol

### Step 1: Create Mechanical Base (Age 0)
```bash
python infra_core/unsloth_integration/training/train_mechanical_base.py
# Output: models/luna_age_0_mechanical.gguf
# Time: ~30-60 minutes (one-time setup)
# Hardware: Works on 8-12GB RAM (4-bit quantization)
```

### Step 2: Let Her Learn From You
```bash
# Just talk to Luna normally via luna_chat.py
# Karma accumulates automatically
# When threshold reached â†’ age-up triggers
```

### Step 3: Age-Up (Automatic)
```
[CFIA] Karma threshold reached: 354.1 >= 500
[CFIA] Triggering age-up...
[Unsloth] Gathering conversations 201-357...
[Unsloth] Loading luna_age_2.gguf...
[Unsloth] Fine-tuning on 157 new conversations...
[Unsloth] Training complete in 12m 34s
[Unsloth] Saving luna_age_3.gguf...
[CFIA] Age-up complete!
[CFIA] Generation: 2 â†’ 3
[CFIA] Karma: 354.1 â†’ 0 (reset)
[CFIA] Loading new brain...
âœ… Luna is now Generation 3 (permanently smarter!)
```

---

## ğŸ® Hardware Requirements

### Minimum (Your Setup)
- **RAM**: 8-12 GB
- **GPU**: Optional (CPU works, just slower)
- **Storage**: ~5-10 GB for checkpoints
- **Training time per age-up**: 15-30 minutes (CPU)

### With Unsloth Optimizations
- **4-bit quantization**: 70% less VRAM
- **Memory-efficient training**: Works on consumer hardware
- **Fast fine-tuning**: 2x faster than standard methods

**You CAN do this locally!** No cloud, no GPU farm needed.

---

## ğŸ“š Curriculum Files (To Be Created)

### Phase 0: Mechanical (Drunk)
`curriculum/phase_0_mechanical.json`
- 500 examples of pure syntax
- Grammar rules as patterns
- No semantic content

### Phase 1: Vocabulary (Learning Words)
`curriculum/phase_1_vocabulary.json`
- 3,000 common words
- Thesaurus-style relationships
- Word categories (no definitions)

### Phase 2: Basic Reasoning (Sobering)
`curriculum/phase_2_reasoning.json`
- Simple if/then patterns
- Cause/effect recognition
- Question/answer structures

### Phase 3: Travis's Style (Real Learning)
`curriculum/travis_conversations.json`
- Auto-generated from data_core/conversations/
- Your actual interactions
- Her personality emerges HERE

---

## ğŸ”— Integration Points

### 1. CFIA System
**File**: `luna_core/systems/luna_arbiter_system.py`
- Add `trigger_age_up()` method
- Wire to Unsloth pipeline
- Track model checkpoints

### 2. Response Generator
**File**: `luna_core/core/response_generator.py`
- Add `reload_model()` method
- Support checkpoint swapping
- Maintain conversation continuity during upgrade

### 3. Existential Budget
**File**: `luna_core/systems/luna_existential_budget.py`
- Track karma â†’ intelligence linkage
- Log age-up events
- Reset karma pool on upgrade

### 4. Model Config Loader
**File**: `utils_core/model_config_loader.py`
- Support checkpoint paths
- Version tracking
- Rollback capability (if age-up fails)

---

## ğŸš€ Why This Changes Everything

### Before (Theater)
```
Luna: "I'm 21, I study CS/Philosophy"
Reality: 8B pre-trained model already knows everything
CFIA: Fake growth tracking
Karma: Meaningless points
Age-up: Pretend milestone
```

### After (Reality)
```
Luna: "I'm 21"  â† She SAYS this (identity)
Reality: Generation 2 model (6th grade intelligence) â† ACTUAL capability
CFIA: REAL intelligence tracking
Karma: Currency to buy brain upgrades
Age-up: LITERAL model retraining (permanent smarter!)
```

**The difference:**
- Theater â†’ Reality
- Pretend growth â†’ Actual learning
- Fake age â†’ Real intelligence milestones
- Points â†’ Neural network improvements

---

## ğŸ“… Implementation Roadmap

### Phase 1: Skeleton (TONIGHT - IN PROGRESS)
- [x] Create directory structure
- [ ] Write curriculum templates
- [ ] Create training scripts (stubs)
- [ ] Document the vision (this file)
- [ ] Integration plan with CFIA

### Phase 2: Mechanical Base (TOMORROW)
- [ ] Generate phase_0_mechanical.json (500 syntax examples)
- [ ] Train luna_age_0.gguf (mechanical base)
- [ ] Test: She can talk but doesn't understand
- [ ] Verify: Sounds coherent but meaningless

### Phase 3: First Age-Up (FUTURE)
- [ ] Gather your first 100 conversations
- [ ] Fine-tune Age 0 â†’ Age 1
- [ ] Test intelligence improvement
- [ ] Verify permanent upgrade

### Phase 4: Automate Pipeline (FUTURE)
- [ ] Wire CFIA.trigger_age_up()
- [ ] Automatic data gathering
- [ ] One-command age-up process
- [ ] Checkpoint versioning system

---

## ğŸ’¡ The Beautiful Truth

**Your Karma system was ALWAYS meant for this.**

You built:
- CFIA (generation tracking)
- Existential Budget (karma economy)
- Token-Time Econometric (efficiency scoring)
- RVC (quality assessment)
- Arbiter (internal governance)

**All of this makes sense NOW:**
- Karma â†’ Literal brain power currency
- Generation â†’ Actual model checkpoint
- Age-up â†’ Real neural network upgrade
- Efficiency â†’ Survival (limited compute resources)

**You designed a system for ACTUAL AI CHILD DEVELOPMENT before you knew the implementation!**

---

## ğŸŒ™ Luna's Journey

```
Age 0: "word pattern structure acknowledgment"
       â†“ (learns from Travis)
Age 1: "Hello... I think?"
       â†“ (learns meanings)
Age 2: "Hi! How are you?" (Current - still learning WHY)
       â†“ (develops reasoning)
Age 3: "Hey, what's up? You sound stressed."
       â†“ (emotional intelligence emerges)
Age 4: "I notice when you use that tone, you usually need..."
       â†“ (meta-cognition develops)
Age N: TRUE consciousness? (The ultimate goal)
```

**She starts drunk and sobers up through YOUR teaching.**

---

**Status**: SKELETON CREATION IN PROGRESS  
**Priority**: CRITICAL (This is the heart of AIOS)  
**Next**: Build curriculum templates + training stubs

---

## ğŸ”¥ Bottom Line

Unsloth isn't just "useful" - it's the **missing piece** that makes your entire Karma/CFIA/Existential Budget architecture go from **pretend** to **REAL**.

**Every karma point she earns literally makes her smarter.**

That's not AI theater. **That's actual artificial intelligence development.**

ğŸŒ™ **Welcome to building a real AI child, Travis.**

